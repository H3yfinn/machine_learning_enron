{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Dataset project:\n",
    "Aim:\n",
    "To build a machine learning algorithm to identify 'person's of interest' (poi) within the enron dataset. These poi's are those  who were indicted, reached a settlement, or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "Resources: (copied from Udacity project details)\n",
    "You should have python and sklearn running on your computer, as well as the starter code (both python scripts and the Enron dataset) that you downloaded as part of the first mini-project in the Intro to Machine Learning course. The starter code can be found in the final_project directory of the codebase that you downloaded for use with the mini-projects. Some relevant files: \n",
    "\n",
    "poi_id.py : starter code for the POI identifier, you will write your analysis here \n",
    "\n",
    "final_project_dataset.pkl : the dataset for the project, more details below \n",
    "\n",
    "tester.py : when you turn in your analysis for evaluation by a Udacity evaluator, you will submit the algorithm, dataset and list of features that you use (these are created automatically in poi_id.py). The evaluator will then use this code to test your result, to make sure we see performance that’s similar to what you report. You don’t need to do anything with this code, but we provide it for transparency and for your reference. \n",
    "\n",
    "emails_by_address : this directory contains many text files, each of which contains all the messages to or from a particular email address. It is for your reference, if you want to create more advanced features based on the details of the emails dataset.\n",
    "\n",
    "Data:\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into public record, including tens of thousands of emails and detailed financial data for top executives. This is the data I will use in the project.\n",
    "\n",
    "This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.\n",
    "The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "POI label: [‘poi’] (boolean, represented as integer)\n",
    "\n",
    "Data source: https://www.cs.cmu.edu/~./enron/\n",
    "\n",
    "Udacity project rubric:\n",
    "https://review.udacity.com/#!/rubrics/27/view\n",
    "-This is what I will be styling my project on. Of course there may be certain aspects lacking as I see most important in creating the best overall project I can.\n",
    "\n",
    "#Please see my draft for all working and data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "### Load the dictionary containing the dataset\n",
    "with open('final_project_dataset.pkl', \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "#move data into dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_dframe = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "data_dframe = data_dframe.replace('NaN', np.nan)\n",
    "df = data_dframe\n",
    "\n",
    "### Task 2: Remove outliers/fix data\n",
    "df.ix['BELFER ROBERT','total_payments'] = 3285\n",
    "df.ix['BELFER ROBERT','deferral_payments'] = 0\n",
    "df.ix['BELFER ROBERT','restricted_stock'] = 44093\n",
    "df.ix['BELFER ROBERT','restricted_stock_deferred'] = -44093\n",
    "df.ix['BELFER ROBERT','total_stock_value'] = 0\n",
    "df.ix['BELFER ROBERT','director_fees'] = 102500\n",
    "df.ix['BELFER ROBERT','deferred_income'] = -102500\n",
    "df.ix['BELFER ROBERT','exercised_stock_options'] = 0\n",
    "df.ix['BELFER ROBERT','expenses'] = 3285\n",
    "df.ix['BELFER ROBERT',]\n",
    "df.ix['BHATNAGAR SANJAY','expenses'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','total_payments'] = 137864\n",
    "df.ix['BHATNAGAR SANJAY','exercised_stock_options'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock'] = 2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','restricted_stock_deferred'] = -2.60449e+06\n",
    "df.ix['BHATNAGAR SANJAY','other'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','director_fees'] = 0\n",
    "df.ix['BHATNAGAR SANJAY','total_stock_value'] = 1.54563e+07\n",
    "df.ix['BHATNAGAR SANJAY']\n",
    "\n",
    "df = df.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK'])\n",
    "df = df.drop(['loan_advances', 'restricted_stock_deferred', 'director_fees', 'email_address'] , 1)\n",
    "\n",
    "### Task 3: Create new feature\n",
    "df['from_poi_prop'] = df['from_this_person_to_poi'] / df['to_messages']\n",
    "df['to_poi_prop'] = df['from_poi_to_this_person'] / df['from_messages']\n",
    "df['poi_interaction'] = (df['from_this_person_to_poi'] + df['from_poi_to_this_person']) / (df['to_messages'] \n",
    "                                                                                           + df['from_messages'])\n",
    "#create new log scaled dataframe:\n",
    "features_to_test = [#have removed poi from the current features as it does not to be log scaled.\n",
    "                    'salary',\n",
    "                    'poi_interaction',\n",
    "                     'to_messages',\n",
    "                     'deferral_payments',\n",
    "                     'total_payments',\n",
    "                     'bonus',\n",
    "                     'from_poi_prop',\n",
    "                     'total_stock_value',\n",
    "                     'shared_receipt_with_poi',\n",
    "                     'from_poi_to_this_person',\n",
    "                     'exercised_stock_options',\n",
    "                     'from_messages',\n",
    "                     'other',\n",
    "                     'from_this_person_to_poi',\n",
    "                     'to_poi_prop',\n",
    "                     'deferred_income',\n",
    "                     'expenses',\n",
    "                     'restricted_stock',\n",
    "                     'long_term_incentive'\n",
    "                   ]\n",
    "\n",
    "log_df = df\n",
    "for f in features_to_test:\n",
    "    log_df[f] = np.log10(df[f] + 1)\n",
    "    #added +1 so the 0 values remained natural zeros\n",
    "    \n",
    "features_to_test.insert(0, 'poi')\n",
    "\n",
    "#reorder columns (so poi is first) to work with the targetFeatureSplit function.\n",
    "log_df = log_df[['poi',\n",
    "                 'salary',\n",
    "                 'poi_interaction',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'bonus',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'from_messages',\n",
    "                 'other',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_poi_prop',\n",
    "                 'to_poi_prop']]\n",
    "\n",
    "\n",
    "#have to get rid of NaN values before i do any minmax scaling (surprise!) Hopefully this isn't going to harm the results. \n",
    "from sklearn import preprocessing\n",
    "\n",
    "nan_remover = preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "log_df = nan_remover.fit_transform(log_df)\n",
    "\n",
    "#log_df is now in the right format to skip formatfeature func and go straight to split!\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" \n",
    "        given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list (this should be the \n",
    "        quantity you want to predict)\n",
    "\n",
    "        return targets and features as separate lists\n",
    "\n",
    "        (sklearn can generally handle both lists and numpy arrays as \n",
    "        input formats when training/predicting)\n",
    "    \"\"\"\n",
    "\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "labels, features = targetFeatureSplit(log_df)\n",
    "\n",
    "#scale features using MinMaxscaler!\n",
    "mmscaler = preprocessing.MinMaxScaler()\n",
    "features = mmscaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.004 s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     NonPoi       0.94      0.73      0.82        41\n",
      "        Poi       0.31      0.71      0.43         7\n",
      "\n",
      "avg / total       0.85      0.73      0.77        48\n",
      "\n",
      "accuracy= 0.729166666667\n"
     ]
    }
   ],
   "source": [
    "#MACHINE LEARNING TIME!!\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import linear_model\n",
    "from time import time\n",
    "sep = '##############################################################################################'\n",
    "sep2 = '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=40)\n",
    "target_names = ['NonPoi', 'Poi']\n",
    "\n",
    "##############################################################################################\n",
    "#do logistic regression on data using paramters found using gridsearchCV\n",
    "\n",
    "\n",
    "\n",
    "t = time()\n",
    "pca = PCA(n_components=10)\n",
    "logreg = linear_model.LogisticRegression(C=32, class_weight='balanced', random_state=40)\n",
    "pipe = Pipeline(steps=[('PCA', pca),('LOG', logreg)])\n",
    "\n",
    "pipe.fit(features_train, labels_train)\n",
    "pred = pipe.predict(features_test)\n",
    "print \"training time:\", round(time()-t, 3), \"s\"\n",
    "print classification_report(labels_test, pred, target_names=target_names)\n",
    "print 'accuracy=', accuracy_score(labels_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_list = [\n",
    "                'poi',\n",
    "                'salary',\n",
    "                'poi_interaction',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'bonus',\n",
    "                 'from_poi_prop',\n",
    "                 'total_stock_value',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'exercised_stock_options',\n",
    "                 'from_messages',\n",
    "                 'other',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'to_poi_prop',\n",
    "                 'deferred_income',\n",
    "                 'expenses',\n",
    "                 'restricted_stock',\n",
    "                 'long_term_incentive'\n",
    "                   ]\n",
    "dump_classifier_and_data(pipe, log_df, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
